---
title: "Project 3: p3package Tutorial"
author: "Joia Zhang"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{p3package Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
The p3package package contains functions for statistical prediction and statistical inference. Included are one sample t-test, linear model, K-nearest neighbors, and random forest functions. Such functions may be useful for conducting hypothesis testing, modeling, and prediction.

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Installation

You can install the p3package package using the following line:

```{r, eval = FALSE}
# install.packages("devtools")
devtools::install_github("joiazhang/p3package", build_vignette = TRUE, build_opts = c())
library(p3package)
```

## Tutorials
```{r}
library(p3package)
```

### `my_t.test` Tutorial
A p-value cut off of $\alpha = 0.05$ will be used in these tutorials.
#### Example 1: A test of the hypothesis
\begin{align}
H_0: \mu &= 60,\\
H_a: \mu &\neq 60.
\end{align}

```{r}
# alternative = two.sided is the default
ttest1 <- my_t.test(x = my_gapminder$lifeExp, mu = 60)
ttest1
```
The p-value (`ttest1$p_val`) is greater than the p-value threshold of $\alpha = 0.05$ so there is not sufficient evidence to reject the null hypothesis. 

#### Example 2: A test of the hypothesis
\begin{align}
H_0: \mu &= 60,\\
H_a: \mu &< 60.
\end{align}

```{r}
ttest2 <- my_t.test(x = my_gapminder$lifeExp, alternative = "less", mu = 60)
ttest2
```
The p-value (`ttest2$p_val`) is less than the p-value threshold of $\alpha = 0.05$ so there is sufficient evidence to reject the null hypothesis. 

#### Example 3: A test of the hypothesis
\begin{align}
H_0: \mu &= 60,\\
H_a: \mu &> 60.
\end{align}

```{r}
ttest3 <- my_t.test(x = my_gapminder$lifeExp, alternative = "greater", mu = 60)
ttest3
```
The p-value (`ttest2$p_val`) is much greater than the p-value threshold of $\alpha = 0.05$ so there is definitely not sufficient evidence to reject the null hypothesis. 


### `my_lm` Tutorial

```{r}
lm_fit <- my_lm(lifeExp ~ gdpPercap + continent, data = my_gapminder)
gdp_coeff <- lm_fit$Coefficients["gdpPercap", "Estimate"]
gdp_p_val <- lm_fit$Coefficients["gdpPercap", "Pr(>|t|)"]
# plot actual vs fitted
mod_fits <- lm_fit$ybar
# create data frame for plotting
my_df <- data.frame(actual = my_gapminder$lifeExp, fitted = mod_fits)
library(ggplot2)
ggplot(my_df, aes(x = fitted, y = actual)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, col = "red", lty = 2) +
  theme_bw(base_size = 15) +
  labs(x = "Fitted values", y = "Actual values", title = "Actual vs. Fitted") +
  theme(plot.title = element_text(hjust = 0.5))
```
While the gdpPercap coefficient is very small `r gdp_coeff`, it is still significant. The null hypothesis for the gdpPercap coefficient is that it does affect life expectancy significantly. The alternative hypothesis is that gdpPercap does affect life expectancy significantly. The p-value for the gdpPercap coefficient is `r gdp_p_val` which is much below the p-value cut off of 0.05. Thus, there is sufficient evidence to reject the null hypothesis and conclude that gdp per capita affects life expectancy.

The Actual vs. Fitted plot shows that the model fit has four clusters. Each cluster is poorly fitted by the model.

### `my_knn_cv` Tutorial
``` {r}
# copy covariates
x <- my_penguins[, c("bill_length_mm", "bill_depth_mm", "flipper_length_mm", "body_mass_g")]
# copy true labels
y <- my_penguins$species
# remove NAs from x and y
temp <- cbind(x, y)
temp <- na.omit(temp)
x <- temp[, -5]
y <- temp[, 5]
# conduct prediction and 5-fold cross validation from k_nn = 1 to 10
knn_list <- list()
cv_err_list <- list()
missclassification_err_list <- list()
library(class)
for (i in 1:10) {
  knn_list[[i]] <- my_knn_cv(train = x, cl = y, k_nn = i, k_cv = 5)
  cv_err_list[[i]] <- knn_list[[i]]$cv_err
  missclassification_err_list[[i]] <- as.double(length(knn_list[[i]]$class[knn_list[[i]]$class == y])) / length(knn_list[[i]]$class)
  missclassification_err_list[[i]] <- as.double(length(knn_list[[i]]$class[knn_list[[i]]$class != y])) / length(knn_list[[i]]$class)
}
err_mat <- cbind(cv_err_list, missclassification_err_list)
colnames(err_mat) <- c("cv", "missclassification")
err_mat
```
Based on the training misclassification rates, k = 1 is the best model. Based on CV misclassification rates, k = 1 is also the best model. In practice, I would choose the model that minimizes the CV misclassifcation rates, rather than the misclassification rates. In cross validation, the misclassification error is calculated for each fold and the CV misclassification error is the mean of the  misclassification errors for each fold. Thus the CV error is able to gauge the error of the predictions based on the training.

### `my_rf_cv` Tutorial
```{r}
set.seed(302)

# list of body mass predictions based on function
library(randomForest)
cv_mse_mat <- matrix(nrow = 30, ncol = 3)
k <- c(2, 5, 10)
for (i in 1:3) {
  for (j in 1:30) {
    cv_mse_mat[j, i] <- my_rf_cv(k[i])
  }
}
cv_mse_df <- data.frame("k2" = cv_mse_mat[, 1], "k5" = cv_mse_mat[, 2], "k10" = cv_mse_mat[, 3])
# box plot for k = 2
ggplot(data = cv_mse_df, aes(x = k2)) + geom_boxplot() + ggtitle("k = 2")
# box plot for k = 5
ggplot(data = cv_mse_df, aes(x = k5)) + geom_boxplot() + ggtitle("k = 5")
# box plot for k = 10
ggplot(data = cv_mse_df, aes(x = k10)) + geom_boxplot() + ggtitle("k = 10")

# create table
average_cv_mse <- colMeans(cv_mse_mat)
sd_cv_mse <- c(sd(cv_mse_mat[, 1]), sd(cv_mse_mat[, 2]), sd(cv_mse_mat[, 3]))
cv_mse_table <- rbind(average_cv_mse, sd_cv_mse)
rownames(cv_mse_table) <- c("average cv mse", "sd")
colnames(cv_mse_table) <- c("k = 2", "k = 5", "k = 10")
cv_mse_table <- as.table(cv_mse_table)
cv_mse_table
```

In the table we can observe that the smaller the value of k, the larger the average CV MSE and the greater the standard deviation. This might be the case because as k (the number of folds) increases, the greater the amount of data is used for training. The more folds there are, the smaller each fold is. Since only one fold is taken out to be the test data, there is more training data available when there are more folds.
